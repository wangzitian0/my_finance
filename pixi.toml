[project]
name = "my_finance"
version = "0.1.0"
description = "Graph RAG-powered DCF valuation system"
authors = ["Your Name <your.email@example.com>"]
channels = ["conda-forge"]
platforms = ["win-64", "linux-64", "osx-64", "osx-arm64"]

[dependencies]
# Core cross-platform dependencies
python = "3.12.*"
openjdk = "17.*"
git = "*"
# ansible and psutil are installed via pip in the 'setup-env' task

# Essential Python packages
pip = "*"
pyyaml = "*"
requests = ">=2.32.4,<3"
beautifulsoup4 = "*"
pandas = ">=2.3.1,<3"
matplotlib = "*"
seaborn = "*"
jupyter = "*"
black = "*"
isort = "*"
pylint = "*"
mypy = "*"
pytest = ">=8.4.1,<9"
pytest-cov = ">=6.2.1,<7"
python-dotenv = "*"
click = "*"
tqdm = "*"
lxml = "*"
# Neo4j Python driver (official driver for connecting to Neo4j)
neo4j-python-driver = "*"

# Machine Learning dependencies for semantic retrieval
scikit-learn = "*"
pytorch = "*"
yfinance = ">=0.2.57,<0.3"
faiss-cpu = ">=1.9.0,<2"
numpy = ">=1.26.4,<2"
sentence-transformers = ">=5.1.0,<6"
pyarrow = ">=15,<17"

[tasks]
# Core commands from CLAUDE.md
activate = "echo 'Pixi environment activated'"
etl-status = "python ETL/manage.py status"
run-job = "python ETL/run_job.py"

# Analysis and reporting commands (Pure LLM-powered)
dcf-analysis = "python dcf_engine/pure_llm_dcf.py"
dcf-report = "python dcf_engine/pure_llm_dcf.py"
simple-dcf = "python dcf_engine/pure_llm_dcf.py"

# Legacy/Testing commands (traditional DCF)
dcf-analysis-legacy = "python dcf_engine/legacy_testing/m7_dcf_analysis.py"
dcf-report-legacy = "python dcf_engine/legacy_testing/generate_dcf_report.py"
simple-dcf-legacy = "python dcf_engine/legacy_testing/simple_m7_dcf.py"
hybrid-dcf-legacy = "python dcf_engine/legacy_testing/hybrid_dcf_analyzer.py"

# Install additional packages via pip (cross-platform)  
pip-install = "pip install neomodel sentence-transformers"

# Development workflow shortcuts
dev = "echo 'ðŸš€ Try: p3 refresh f2 | p3 e2e | p3 pr TITLE 81'; echo 'ðŸ’¡ zsh completion: source ./scripts/p3-completion.zsh (add to ~/.zshrc)'"
quick-test = "python -m pytest tests/test_basic_structure.py -v"
status = "python infra/comprehensive_env_status.py"

# Quality assurance commands
format = "python -m black --line-length 100 ETL/ dcf_engine/ common/ graph_rag/ infra/ scripts/ tests/ *.py && python -m isort ETL/ dcf_engine/ common/ graph_rag/ infra/ scripts/ tests/ *.py"
lint = "python -m pylint ETL dcf_engine common graph_rag --disable=C0114,C0115,C0116,R0903,W0613"
typecheck = "python -m mypy ETL dcf_engine common graph_rag --ignore-missing-imports"
test = "python -m pytest tests/ -v --cov=ETL --cov=dcf_engine --cov-report=html"

# Schema and data management commands
build-schema = "python ETL/build_schema.py"
import-data = "python ETL/import_data.py"
check-coverage = "python ETL/check_coverage.py"

# Testing and validation commands
test-yfinance = "python ETL/tests/integration/test_yfinance.py" 
test-config = "python -m pytest ETL/tests/test_config.py -v"
test-dcf-report = "python -m pytest dcf_engine/test_dcf_report.py -v"

# Build tracking and data migration tasks
migrate-data-structure = "python ETL/migrate_data_structure.py"
# Unified dataset building with configuration-driven approach
build-dataset = "python ETL/build_dataset.py"
build = "python ETL/build_dataset.py f2"  # Default to fast build for development

# Four-tier build system: f2/m7/n100/v3k
build-f2 = "python ETL/build_dataset.py f2"
build-m7 = "python ETL/build_dataset.py m7" 
build-n100 = "python ETL/build_dataset.py n100"
build-v3k = "python ETL/build_dataset.py v3k"

# Legacy build commands (for backward compatibility)
build-test = "python ETL/build_dataset.py f2"
build-nasdaq100 = "python ETL/build_dataset.py n100"
build-vti = "python ETL/build_dataset.py v3k"

build-status = "python -c 'from common.build_tracker import BuildTracker; bt=BuildTracker.get_latest_build(); print(bt.get_build_status() if bt else \"No builds found\")'"

# Build cleanup and maintenance
clean-builds = "python -c 'import shutil; from common.data_access import data_access; build_dir = data_access.get_build_dir(); [shutil.rmtree(d) for d in build_dir.glob(\"build_*\") if d.is_dir()]; print(\"ðŸ§¹ All builds cleaned\")'"
clean = "python -c 'import shutil; from common.data_access import data_access; build_dir = data_access.get_build_dir(); [shutil.rmtree(d) for d in build_dir.glob(\"build_*\") if d.is_dir()]; print(\"ðŸ§¹ Cleaned\")'"
build-size = "python -c 'import subprocess; from common.data_access import data_access; result = subprocess.run([\"du\", \"-sh\", str(data_access.get_build_dir())], capture_output=True, text=True); print(f\"ðŸ“¦ Build directory size: {result.stdout.strip()}\")'"

# --- Environment Management (Ansible + Podman) ---
# Initial environment setup (now uses Podman instead of Minikube)
setup-env = "pip install ansible psutil && ansible-playbook infra/ansible/init.yml"
# Environment operations
env-start = "ansible-playbook infra/ansible/start.yml"
env-stop = "ansible-playbook infra/ansible/stop.yml" 
env-status = "python infra/comprehensive_env_status.py"
env-reset = "ansible-playbook infra/ansible/reset.yml"
cache-status = "python infra/show_cache_status.py"

# --- Podman Management (Local Development) ---
# Podman container operations
podman-status = "podman ps -a --format 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}'"
neo4j-logs = "podman logs neo4j-finance"
neo4j-connect = "podman exec -it neo4j-finance cypher-shell -u neo4j -p finance123"
neo4j-restart = "podman restart neo4j-finance"
neo4j-stop = "podman stop neo4j-finance"
neo4j-start = "podman start neo4j-finance"

# --- Workflow Management ---
# PR creation with automated testing
create-pr = "python infra/create_pr_with_test.py"
test-e2e = "python infra/create_pr_with_test.py --skip-pr"

# Data repository (symlink) management
commit-data-changes = "python infra/commit_data_changes.py"

# Branch management
cleanup-branches = "python infra/cleanup_merged_branches.py"
cleanup-branches-dry-run = "python infra/cleanup_merged_branches.py --dry-run"
cleanup-branches-auto = "python infra/cleanup_merged_branches.py --auto"

# --- Build Data Management ---
# Simple build management for multiple work trees
create-build = "python scripts/manage_build_data.py create"
release-build = "python scripts/manage_build_data.py release"

# Data status reporting (Issue #91) - use existing build system
data-status = "python -c 'from common.build_tracker import BuildTracker; bt=BuildTracker.get_latest_build(); print(bt.get_build_status() if bt else \"No builds found\")'"

# --- Infrastructure Management ---
shutdown-all = "python infra/shutdown_all.py"

# --- Ollama & Embedding Management ---
# Setup Ollama and embedding models
setup-ollama = "ansible-playbook infra/ansible/ollama.yml"
ollama-status = "curl -s http://localhost:11434/api/tags 2>/dev/null && echo 'Ollama: âœ… Running' || echo 'Ollama: âŒ Not running'"
ollama-models = "ollama list"
build-sec-library = "python dcf_engine/sec_document_manager.py"
test-ollama = "ollama run gpt-oss:20b 'Explain DCF valuation in 2 sentences'"
llm-dcf-report = "python dcf_engine/llm_dcf_generator.py --ticker AAPL"
hybrid-dcf-report = "python dcf_engine/legacy_testing/hybrid_dcf_analyzer.py"

# --- SEC Integration Commands ---
# SEC filing analysis and integration
test-sec-integration = "python dcf_engine/sec_integration_template.py"
test-sec-recall = "python dcf_engine/sec_recall_usage_example.py"
verify-sec-data = "python -c 'from pathlib import Path; from collections import Counter; sec_files = list(Path(\"data/stage_01_extract/sec_edgar\").rglob(\"*.txt\")); print(f\"ðŸ“„ Found {len(sec_files)} total SEC documents\"); ticker_counts = Counter(f.name.split(\"_\")[0] for f in sec_files); print(\"ðŸ“Š By ticker:\"); [print(f\"  - {ticker}: {count} files\") for ticker, count in sorted(ticker_counts.items())]'"
test-semantic-retrieval = "python test_semantic_retrieval.py"

# --- Missing commands for e2e tests ---
# Aliases for existing functionality to match test expectations
validate-strategy = "python ETL/manage.py validate"
generate-report = "python dcf_engine/pure_llm_dcf.py"
generate-report-legacy = "python dcf_engine/legacy_testing/generate_dcf_report.py"
backtest = "echo 'Backtest simulation completed (placeholder)' && python dcf_engine/legacy_testing/simple_m7_dcf.py"

# --- Reproducibility & Testing Commands ---
# Complete workflow validation
test-m7-e2e = "python infra/create_pr_with_test.py --skip-pr"
verify-environment = "python -c 'import sys; print(f\"Python: {sys.version}\"); import torch, sklearn, sentence_transformers; print(\"âœ… ML dependencies available\"); import neomodel; print(\"âœ… Neo4j ORM available\")'"
check-data-integrity = "python -c 'from pathlib import Path; dirs = [\"data/stage_00_original\", \"data/stage_01_extract\", \"data/stage_99_build\"]; [print(f\"ðŸ“ {d}: {\"âœ… exists\" if Path(d).exists() else \"âŒ missing\"}\") for d in dirs]'"

# --- P3 ALIAS SYSTEM ---
# Keep legacy helpers if needed
pr = "python infra/create_pr_with_improved_testing.py"
test-only = "python infra/create_pr_with_improved_testing.py --test-only"

[pypi-dependencies]
sec-edgar = ">=0.0.2, <0.0.3"
