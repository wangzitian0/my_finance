# Data Quality Reports

This directory contains automated data quality assessment reports and metrics.

## Purpose

Data quality monitoring and reporting for:
- ETL pipeline data validation
- Source data quality assessment  
- Build artifact quality metrics
- Trend analysis and anomaly detection

## Report Types

### Pipeline Quality Reports
- **Data Completeness**: Missing data detection and analysis
- **Data Consistency**: Cross-source validation and consistency checks
- **Data Freshness**: Timeliness and update frequency analysis
- **Data Accuracy**: Validation against expected ranges and formats

### Source-Specific Reports
- **YFinance Quality**: Stock data quality and completeness
- **SEC Edgar Quality**: Filing availability and parsing success
- **Neo4j Quality**: Graph database integrity and relationships
- **Build Quality**: Build artifact validation and metrics

## Report Structure

### Standard Metrics
- Data completeness percentages
- Error counts and error rates
- Processing performance metrics
- Data freshness timestamps

### Quality Scores
- Overall data quality score (0-100)
- Source-specific quality ratings
- Trend analysis and change detection
- Threshold-based alerting metrics

## Generated Reports

Reports are generated by:
- `common/quality_reporter.py` - Quality assessment framework
- ETL pipeline modules - Source-specific quality checks
- Build system - Artifact validation reports
- Scheduled monitoring jobs - Periodic quality assessment

## Usage

### Manual Quality Checks
```bash
# Generate quality report for latest build
p3 build-status

# Check data integrity across all sources
p3 check-integrity

# Validate specific dataset quality
python common/quality_reporter.py --dataset m7
```

### Automated Monitoring
- Quality reports are generated automatically during builds
- Threshold violations trigger alerts
- Historical trends are tracked for regression detection
- Integration with build validation workflows

## Quality Thresholds

### Standard Thresholds
- **Data Completeness**: > 95% for production datasets
- **Error Rate**: < 1% for stable data sources
- **Processing Success**: > 98% for ETL pipelines
- **Data Freshness**: Within expected update windows

### Custom Thresholds
- Source-specific quality requirements
- Dataset-tier specific standards (F2/M7/N100/V3K)
- Business logic validation rules
- Performance benchmarks and SLA requirements

## Alert Configuration

Quality monitoring includes:
- **Threshold Violations**: Automatic alert generation
- **Trend Degradation**: Quality trend analysis and alerts
- **Data Anomalies**: Statistical outlier detection
- **Pipeline Failures**: ETL error tracking and notification

## Historical Analysis

Quality reports support:
- **Trend Analysis**: Quality metrics over time
- **Regression Detection**: Quality degradation identification
- **Comparative Analysis**: Cross-dataset quality comparison
- **Root Cause Analysis**: Error pattern analysis and investigation

## Integration

Quality reports integrate with:
- Build system validation workflows
- PR creation and testing processes
- Automated deployment pipelines
- Monitoring and alerting systems

## Best Practices

1. **Regular Monitoring**: Review quality reports regularly
2. **Threshold Management**: Adjust thresholds based on experience
3. **Trend Analysis**: Focus on trends rather than point-in-time metrics
4. **Root Cause Investigation**: Investigate quality degradations promptly
5. **Documentation**: Document quality issues and resolutions